{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e0e5d8-9e52-4be4-9451-9398c6f6f057",
   "metadata": {},
   "source": [
    "# <font color='Blood Red'>**Attrition & Promotion Prediction System**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6b4809-526c-4a03-b759-2345ae2538d8",
   "metadata": {},
   "source": [
    "## <font color='Blue'>**CAZA Dataset**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866e4df-508e-4b86-9397-b034883c64c0",
   "metadata": {},
   "source": [
    "## <font color='#21d12a'>**_Predict Why Employees Quit & why get Promoted_**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00426d2c-443a-4d0b-9886-cf3628108509",
   "metadata": {},
   "source": [
    "![title](pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8955caf-4dba-499d-828f-2d0422afaac5",
   "metadata": {},
   "source": [
    "## 📌 <font color='#21d12a'>*Before starting the analysis, it's important to define the business questions and understand the structure of each row in the dataset. Here's how we can approach it:*</font> \n",
    "\n",
    "<font size=\"4\">**_1. Business Question:_**</font>\n",
    "\n",
    "<font size=\"4\">_The business question defines the purpose of analyzing the data. It could focus on employee performance, attrition, promotions, and workforce management._</font>\n",
    "\n",
    ">- Attrition Prediction: What factors are driving employee attrition? Can we predict whether an employee will leave the organization based on features like salary, years of experience, or KPIs?\n",
    "Promotion Factors: What are the key factors contributing to employee promotion? How do training, education, and performance metrics affect promotion rates?\n",
    "\n",
    ">- Employee Retention: How can we increase employee retention? What are the most significant factors (e.g., department, job role, training, salary) that contribute to higher retention rates?\n",
    "\n",
    ">- Training Effectiveness: How does training influence employee performance, as reflected in KPIs met, promotions, or salary increments?\n",
    "\n",
    ">- Salary Discrepancies: Are there any discrepancies in salary based on department, gender, or position? How does the gross salary differ across regions and positions?\n",
    "\n",
    "<font size=\"4\">**_2. Representation of Each Row:_**</font>\n",
    "\n",
    "<font size=\"4\">_Each row in the dataset represents an individual employee with attributes that describe their demographics, employment status, performance metrics, compensation, and engagement with the organization._</font>\n",
    "\n",
    "<font size=\"4\">_Breakdown of each row:_</font>\n",
    "\n",
    ">- **1- Employee_id:** A unique identifier for the employee.\n",
    ">- **2- Department:** The department in which the employee works (e.g., Finance, IT).\n",
    ">- **3- Position:** The employee’s role within the organization (e.g., Manager, Engineer).\n",
    ">- **4- Education:** The highest level of education achieved by the employee.\n",
    ">- **5- Region:** The geographic region where the employee is located.\n",
    ">- **6- Recruitment_channel:** The method through which the employee was hired (e.g., sourcing, referral).\n",
    ">- **7- Years_of_trainings:** Number of years the employee has undergone training.\n",
    ">- **8- Previous_Year_Rating:** The employee's performance rating for the previous year.\n",
    ">- **9- KPIs_met >80%:** Whether the employee met more than 80% of their KPIs (Key Performance Indicators).\n",
    ">- **10- Awards_Won:** Whether the employee has won any awards.\n",
    ">- **11- Avg_Training_Score:** The average score of the employee's training assessments.\n",
    ">- **12- DOB:** The employee's date of birth.\n",
    ">- **13- Gender:** The gender of the employee.\n",
    ">- **14- Marital_Status:** The marital status of the employee (e.g., Single, Married).\n",
    ">- **15- Dependents:** The number of dependents the employee has.\n",
    ">- **16- DOJ:** The date the employee joined the company.\n",
    ">- **17- Salary:** The employee's base salary.\n",
    ">- **18- HRA:** House Rent Allowance, a part of the employee's salary.\n",
    ">- **19- DA:** Dearness Allowance, another salary component.\n",
    ">- **20- PF:** Provident Fund, a retirement savings scheme.\n",
    ">- **21- Gross_Salary:** The total salary before deductions.\n",
    ">- **22- Insurance:** The type of insurance coverage the employee has (e.g., medical, life).\n",
    ">- **23- Over_Time:** Whether the employee works overtime.\n",
    ">- **24- Business_Travel:** The employee’s frequency of business travel.\n",
    ">- **25- Attrition:** Whether the employee has left the organization (yes/no).\n",
    ">- **26- Promotion:** Whether the employee has been promoted.\n",
    "\n",
    "<font size=\"4\">_This structure provides a comprehensive view of the employee, allowing for detailed analysis of performance, compensation, career progression, and more._</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd80fd-a20d-403b-9668-9157849cd58c",
   "metadata": {},
   "source": [
    "## 📌 <font color='#21d12a'>*Target Tasks:*</font> \n",
    "<font size=\"4\">**_1- Project Understanding:_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_2- Data Preparation_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_3- Mining & EDA (Exploratory of Data Analysis)_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_4- Data Preprocessing_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_5- Model Data_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_6- Intrepret Results_**</font>\n",
    "<br>\n",
    "<font size=\"4\">**_7- Deployment_**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac39c77-2aba-40f6-ba53-e3507f8fbc9d",
   "metadata": {},
   "source": [
    "# 📌 **_Step 1- Project Understanding:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a7cbd-b76a-4029-b137-72410bd72602",
   "metadata": {},
   "source": [
    "<font size=\"4\">_For this project, the focus is on analyzing employee data to answer key business questions related to performance, retention, salary, and career progression. The dataset includes 35,969 rows and 27 columns, representing individual employees and their associated attributes, such as demographics, compensation, performance, and employment status._</font>\n",
    "\n",
    "<font size=\"4\">**_1. Objective:_**</font>\n",
    "\n",
    "<font size=\"4\">**_The main goal of this project is to derive insights from the employee dataset that can help the company improve its workforce management. Potential objectives include:_**</font>\n",
    "\n",
    ">- Identifying factors that drive employee attrition and ways to reduce it.\n",
    ">- Determining the factors that contribute to employee promotion.\n",
    ">- Analyzing salary trends to ensure fair compensation across departments, positions, and demographics.\n",
    ">- Understanding the effectiveness of training and its impact on employee performance (KPIs, awards).\n",
    ">- Studying career progression patterns across the organization.\n",
    "\n",
    "<font size=\"4\">**_2. Key Questions:_**</font>\n",
    "\n",
    ">- **Attrition:** What factors are most strongly correlated with employees leaving the company? Can we build a predictive model to identify employees at risk of attrition?\n",
    ">- **Promotions:** What factors contribute to employee promotion? Does education, department, or performance play a larger role in determining promotion?\n",
    ">- **Salary Analysis:** Are there any discrepancies in salary distribution based on department, gender, or position? How do different components like HRA, DA, and PF affect gross salary?\n",
    ">- **Training and Performance:** How does the number of years of training and training scores influence an employee’s overall performance (KPIs met, awards won, promotions)?\n",
    "\n",
    "<font size=\"4\">**_3. Data Breakdown:_**</font>\n",
    "\n",
    "<font size=\"4\">**_Each row represents an employee with a variety of attributes:_**</font>\n",
    "\n",
    ">- **Demographics:** Gender, marital status, dependents, region.\n",
    ">- **Performance Metrics:** KPIs met, awards won, previous year rating, average training score.\n",
    ">- **Career Progression:** Department, position, years of experience, promotions.\n",
    ">- **Compensation:** Salary, HRA, DA, PF, gross salary, insurance type.\n",
    ">- **Other Factors:** Business travel, overtime, recruitment channel, training details.\n",
    "\n",
    "<font size=\"4\">**_4. Business Impact:_**</font>\n",
    "\n",
    "<font size=\"4\">**_The analysis will provide insights into:_**</font>\n",
    "\n",
    ">- **Employee Retention:** By identifying patterns in attrition, the company can implement strategies to retain key employees and reduce turnover costs.\n",
    ">- **Performance Improvement:** Understanding the relationship between training, KPIs, and awards can help improve overall employee performance.\n",
    ">- **Fair Compensation:** Salary discrepancies based on demographics or departments can be addressed, ensuring that compensation is fair and competitive.\n",
    ">- **Promotion Strategy:** A clear understanding of promotion patterns will allow the company to ensure that high-performing employees are being rewarded and promoted appropriately.\n",
    "\n",
    "<font size=\"4\">**_5. Potential Analytical Approaches:_**</font>\n",
    "\n",
    "<font size=\"4\">**_Exploratory Data Analysis (EDA): To understand the overall trends in the data, such as the distribution of salaries, years of experience, and attrition rates._**</font>\n",
    "\n",
    ">- **Correlation Analysis:** To examine relationships between variables, such as how training and performance metrics are correlated with promotions.\n",
    ">- **Predictive Modeling:** Build models (e.g., logistic regression, decision trees) to predict employee attrition or promotion likelihood based on the available data.\n",
    ">- **Salary Analysis:** Perform group comparisons (e.g., by department, gender, region) to identify any salary discrepancies.\n",
    "\n",
    "<font size=\"4\">**_6. Challenges:_**</font>\n",
    "\n",
    ">- **Missing Data:** Some rows may have missing or incomplete information, requiring imputation or exclusion of such records.\n",
    ">- **Data Imbalance:** Certain categories (e.g., promotions, awards) may be imbalanced, necessitating appropriate handling during analysis.\n",
    ">- **Feature Selection:** Identifying the most relevant features (e.g., salary, training, experience) for specific analyses like promotion and attrition will be crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e75ebc-685b-4a19-b08d-f92135aa52d4",
   "metadata": {},
   "source": [
    "# 📌 **_Step 2- Data Preparation:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372caf4-6d41-4664-9296-cd1d08e9834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.pandas\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale,StandardScaler, MinMaxScaler, Normalizer, RobustScaler, KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest, RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import shap\n",
    "import joblib # Added joblib for saving models\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a4be0-3a5d-4b18-ab3b-0248bbe39557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('CAZA_Employee_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16847ebe-dabe-4bdd-87be-5cac42fe5faa",
   "metadata": {},
   "source": [
    "📌 <font size=\"4\" font color='red'>**_Warning Description_**</font>\n",
    "\n",
    "<font size=\"4\">*_This warning indicates that certain columns in our dataset have mixed data types (e.g., strings mixed with numbers), which can cause issues during data processing. This can happen in case, for example, some rows in a numerical column contain text or missing values are inconsistently represented, there are steps to address the warning as following:._*</font>\n",
    "\n",
    ">- <font size=\"4\">**_a. Set low_memory=False:_**</font>\n",
    "\n",
    "> **_\"Load the dataset with low_memory=False\"_**\n",
    "> </br>\n",
    "> df = pd.read_csv('CAZA_Employee_Dataset.csv', low_memory=False)\n",
    "\n",
    "<font size=\"4\">_This option ensures that the entire file is read into memory at once, allowing pandas to properly infer the data types._</font>\n",
    "\n",
    ">- <font size=\"4\">**_b. Specify Data Types Explicitly:_**</font>\n",
    "\n",
    "> **_\"Explicitly define the data types for the problematic columns\"_**\n",
    "> </br>\n",
    "> dtype_dict = {\n",
    "> </br>\n",
    ">   'column_name_11': 'str',\n",
    "> </br>\n",
    ">   'column_name_13': 'str',\n",
    "> </br>\n",
    ">    (Add more columns as needed...)\n",
    "> </br>\n",
    ">    }\n",
    "> </br>\n",
    "> **_\"Load the dataset with the specified data types\"_**\n",
    "> </br>\n",
    "> df = pd.read_csv('Ready_Employee_Dataset.csv', dtype=dtype_dict).\n",
    "\n",
    "<font size=\"4\">*_This option can explicitly set the data types for the problematic columns when loading the csv file. This can prevent pandas from misinterpreting the data types._*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5e360-ff27-4b01-bd9d-3be649914a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with low_memory=False\n",
    "df = pd.read_csv('CAZA_Employee_Dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a7e75-e04e-432f-be29-0554fb7f5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58a936-6592-4d62-b407-006a80dfee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b614c9-3065-4ee2-9feb-9fc7312c8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates based on Employee_id\n",
    "duplicate_rows = df[df.duplicated(subset=['Employee_id'], keep=False)]\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.shape[0]\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# Optionally, export the duplicates to a new CSV file\n",
    "duplicate_rows.to_csv('1-Duplicates_Full_CAZA_Emp_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb551d04-7ae0-4c8a-833b-bcc39ad3bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows before dropping duplicates\n",
    "initial_count = df.shape[0]\n",
    "\n",
    "# Drop duplicates based on Employee_id, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['Employee_id'], keep='first')\n",
    "\n",
    "# Count the number of rows after dropping duplicates\n",
    "final_count = df_cleaned.shape[0]\n",
    "\n",
    "# Calculate the number of dropped duplicates\n",
    "dropped_count = initial_count - final_count\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_cleaned.to_csv('2-DroppedDup_Full_CAZA_Emp_Dataset.csv', index=False)\n",
    "\n",
    "print(f\"Number of dropped duplicate rows: {dropped_count}\")\n",
    "print(\"Cleaned dataset saved as 'DroppedDup_Full_CAZA_Emp_Dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a82c9-298c-4db9-9492-d2821d5804fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with low_memory=False\n",
    "df = pd.read_csv('2-DroppedDup_Full_CAZA_Emp_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525c02e-e18d-4e69-8c4a-e60cddf6bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert numeric columns to int (handling NaN first)\n",
    "columns_to_int = ['Years_of_trainings', 'Previous_Year_Rating', 'KPIs_met >80%', 'Awards_Won', 'Avg_Training_Score', 'Dependents']\n",
    "\n",
    "# Fill NaN with a placeholder (e.g., -1) before conversion\n",
    "df[columns_to_int] = df[columns_to_int].fillna(-1).astype(int)\n",
    "\n",
    "columns_to_positive = ['Previous_Year_Rating','Dependents']\n",
    "df[columns_to_positive] = df[columns_to_positive].replace(-1, 1)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv('3-Preprocessed_Full_CAZA_Emp_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1238dea-3957-4080-b996-82ccec3282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with low_memory=False\n",
    "df = pd.read_csv('3-Preprocessed_Full_CAZA_Emp_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf4448-84c6-4e5d-9eeb-814f0b1da409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ab318-d2e3-4b2e-97ce-5850c5cbf46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rows Based on Target Columns:\n",
    "# Remove Rows with Missing Target Values: Since \"Attrition\" and \"Promotion\" are our target columns.\n",
    "# We should remove any rows where these are missing.\n",
    "\n",
    "# Remove rows where Attrition or Promotion is missing\n",
    "df = df.dropna(subset=['Attrition', 'Promotion'])\n",
    "\n",
    "# Save the cleaned dataset (optional)\n",
    "df.to_csv('4-RemoveMissingTarget_Full_CAZA_Emp_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96549eeb-df0c-45cd-a038-94784bb9b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with low_memory=False\n",
    "df = pd.read_csv('4-RemoveMissingTarget_Full_CAZA_Emp_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299375d-890b-444f-90a4-bc6c21b31878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove 'L.E.' and commas from Salary and convert it to float\n",
    "df['Salary'] = df['Salary'].str.replace('L.E.', '', regex=False).str.replace(',', '').str.strip()\n",
    "df['Salary'] = df['Salary'].astype(float)\n",
    "\n",
    "# Step 2: Remove commas and convert each column of HRA, DA, PF and Gross Salary to float\n",
    "# List of columns to clean and convert to float\n",
    "columns_to_clean = ['HRA', 'DA', 'PF', 'Gross_Salary']\n",
    "for column in columns_to_clean:\n",
    "    df[column] = df[column].str.replace(',', '').str.strip().astype(float)\n",
    "\n",
    "# Step 3: Remove symbols from the Department column\n",
    "symbols_to_remove = r'[~~$ԢԹԪ]'  # Regular expression for symbols to remove\n",
    "df['Department'] = df['Department'].apply(lambda x: re.sub(symbols_to_remove, '', str(x)))\n",
    "\n",
    "# Step 4: Remove extra spaces in Department and Position columns\n",
    "df['Department'] = df['Department'].str.strip().replace(r'\\s+', ' ', regex=True)  # Remove leading, trailing, and excessive spaces\n",
    "df['Position'] = df['Position'].str.strip().replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Step 5: Convert columns to datetime\n",
    "df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n",
    "df['DOJ'] = pd.to_datetime(df['DOJ'], errors='coerce')\n",
    "\n",
    "# Step 6: Impute Categorical columns (impute with mode)\n",
    "categorical_cols = ['Education','Marital_Status', 'Insurance']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Save the cleaned dataset (optional)\n",
    "df.to_csv('5-Cleaned_Preprocessed_Full_CAZA_Emp_Dataset.csv', index=False)\n",
    "\n",
    "# Check the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2912f0-683f-44a7-ba49-a5cfefcd32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with low_memory=False\n",
    "df = pd.read_csv('5-Cleaned_Preprocessed_Full_CAZA_Emp_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ecd2f-3070-4fc5-80bb-b3a2d07f79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "# ......................................\n",
    "\n",
    "# Ensure DOB and DOJ are in datetime format\n",
    "df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n",
    "df['DOJ'] = pd.to_datetime(df['DOJ'], errors='coerce')\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Step 1: Generate Years_In_Company\n",
    "df['Years_In_Company'] = (today - df['DOJ']).dt.days // 365  # Convert the difference to years\n",
    "\n",
    "# Step 2: Generate Age\n",
    "df['Age'] = (today - df['DOB']).dt.days // 365  # Convert the difference to years\n",
    "\n",
    "# Step 3: Generate Years_of_Experience\n",
    "# Get today's date\n",
    "today = pd.to_datetime('31/05/2024', format='%d/%m/%Y')\n",
    "\n",
    "# Calculate the difference in years\n",
    "df['Years_of_Experience'] = today.year - df['DOB'].dt.year - 21\n",
    "\n",
    "# Adjust the experience for those whose birthday hasn't occurred yet this year\n",
    "has_birthday_passed = (today.month > df['DOB'].dt.month) | ((today.month == df['DOB'].dt.month) & (today.day >= df['DOB'].dt.day))\n",
    "df['Years_of_Experience'] -= ~has_birthday_passed\n",
    "\n",
    "# Save the updated dataset (optional)\n",
    "df.to_csv('6-Ready_Emp_Dataset.csv', index=False)\n",
    "\n",
    "# Check the result\n",
    "print(df[['DOJ', 'Years_In_Company', 'DOB', 'Age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045734c1-8812-4aa7-8729-c18849757e27",
   "metadata": {},
   "source": [
    "📌 <font size=\"4\" font color='Green'>**_Normalizing categorical columns_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fa5af-4751-49bb-b2ed-0639bdc1c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to normalize\n",
    "categorical_columns = [\n",
    "    'Department', 'Position', 'Education', 'Region', 'Recruitment_channel', \n",
    "    'Gender', 'Marital_Status', 'Insurance', 'Over_Time', \n",
    "    'Business_Travel', 'Attrition', 'Promotion'\n",
    "]\n",
    "\n",
    "# Apply string methods to each categorical column\n",
    "for col in categorical_columns:\n",
    "    if df[col].dtype == 'object':  # Ensure the column is of type 'object'\n",
    "        df[col] = df[col].str.strip().str.title()\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv('7-Ready_Emp_Dataset.csv', index=False)\n",
    "\n",
    "print(\"Categorical columns had been normalized and saved to '7-Ready_Emp_Dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19474db1-5cbb-4e2e-aa97-f121ff543ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Get the info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9261d-f63a-468f-abf3-34e495520303",
   "metadata": {},
   "source": [
    "# 📌 **_Step 3- Data Mining & EDA (Exploratory of Data Analysis):_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2d3b3-f994-47e5-a4c1-c631608d2aca",
   "metadata": {},
   "source": [
    "<font size=\"4\">**_Data Mining Overview_**</font>\n",
    "\n",
    "<font size=\"4\">_Data mining refers to extracting useful patterns, insights, or models from large datasets. In this project, the goal of data mining is to discover patterns that explain employee behaviors, such as attrition, promotions, or salary trends, and to predict outcomes based on those patterns._</font>\n",
    "\n",
    "<font size=\"4\">_Once the dataset is prepared, perform EDA to gain insights:_</font>\n",
    "\n",
    ">- **Visualizations:** Create visualizations (e.g., histograms, boxplots, heatmaps) to explore relationships and distributions.\n",
    ">- **Correlations:** Check correlations between variables like salary, years of experience, and attrition.\n",
    ">- **Group Analysis:** Analyze key features like promotion rates, attrition, or salary distribution across departments or regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdff98-76e7-41af-a3df-82529c2906db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Describe your dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8feba6-fade-41c9-b546-6719739481d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{column}: Number of unique values {df[column].nunique()}\")\n",
    "    print(\"==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be071fa-a7b3-40ff-862b-5bdff941b08e",
   "metadata": {},
   "source": [
    "## 📌 **_Distribution Analysis:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49cdba-8be2-427f-9add-5a470682aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1.1 Histogram for Age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Age'], kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141727d-3220-411f-8dc1-e81903599956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Boxplot for Salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Salary', data=df)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b571c-8f7d-4bc1-bd1b-a2b149d332ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Heatmap for correlations between numerical features\n",
    "\n",
    "# Select only the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix for Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008cff7-e5c1-4075-a514-f877d6b0a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Group analysis by Department\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Department', y='Salary', data=df)\n",
    "plt.title('Salary Distribution Across Departments')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5660c2-4c26-4371-95bb-c3ad57d3dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Group analysis by Region\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Region', y='Salary', data=df)\n",
    "plt.title('Salary Distribution Across Regions')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa27d9-f691-41f2-8980-8b67255ca17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Salary and Gender\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Gender', y='Salary', data=df)\n",
    "plt.title('Distribution between Salary and Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cf37d-ab7a-4893-bb8d-4827b46bad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Salary and Position\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='Position', y='Salary', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution between Salary and Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af89b5-19dc-437f-b665-49244135e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Gender and Marital Status\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Marital_Status', hue='Gender', data=df)\n",
    "plt.title('Distribution between Gender and Marital Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6dd52-575b-40d5-8264-4930fea86ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to match in positions\n",
    "keywords = [\n",
    "    'Director', 'Regional', 'Head', 'Manager', 'Executive', 'Associate', \n",
    "    'Assistant', 'Engineer', 'Developer', 'Accountant', 'Lawyer', 'Paralegal', \n",
    "    'Analyst', 'Representative', 'Intern', 'Specialist', 'Junior'\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any of the keywords\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "# Filter the DataFrame for positions containing any of the keywords\n",
    "filtered_df = df[df['Position'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "# Calculate the average salary for each position\n",
    "avg_salary_by_position = filtered_df.groupby('Position')['Salary'].mean()\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "all_positions_df = pd.DataFrame(index=filtered_df['Position'].unique())\n",
    "all_positions_df['Average_Salary'] = avg_salary_by_position\n",
    "\n",
    "# Fill NaN values with 0 or another placeholder if you prefer\n",
    "all_positions_df['Average_Salary'] = all_positions_df['Average_Salary'].fillna(0)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "all_positions_df['Average_Salary'].sort_values().plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.title('Average Salary by Position')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc317c-14a9-4a47-965e-a289f0b59370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Define groups of keywords for the positions\n",
    "group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "\n",
    "# Function to categorize positions based on keywords\n",
    "def categorize_position(position):\n",
    "    if any(keyword in position for keyword in group_1):\n",
    "        return 'Group 1 (Director, Regional, Head, Manager, Executive)'\n",
    "    elif any(keyword in position for keyword in group_2):\n",
    "        return 'Group 2 (Associate, Assistant, Engineer)'\n",
    "    elif any(keyword in position for keyword in group_3):\n",
    "        return 'Group 3 (Developer, Accountant, Lawyer)'\n",
    "    elif any(keyword in position for keyword in group_4):\n",
    "        return 'Group 4 (Representative, Paralegal, Analyst)'\n",
    "    elif any(keyword in position for keyword in group_5):\n",
    "        return 'Group 5 (Intern, Specialist, Junior)'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to categorize positions\n",
    "df['Position_Group'] = df['Position'].apply(categorize_position)\n",
    "\n",
    "# Filter out the 'Other' group to focus on the specified positions\n",
    "df_filtered = df[df['Position_Group'] != 'Other']\n",
    "\n",
    "# Convert salary to numeric if it's not already\n",
    "# df_filtered['Salary'] = pd.to_numeric(df_filtered['Salary'].str.replace('L.E.', ''), errors='coerce')\n",
    "\n",
    "# Calculate the average salary for each position group\n",
    "avg_salary = df_filtered.groupby('Position_Group')['Salary'].mean().reset_index()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Position_Group', y='Salary', data=avg_salary, palette='Set2')\n",
    "\n",
    "# Add numbers on the bars\n",
    "for index, row in avg_salary.iterrows():\n",
    "    plt.text(index, row.Salary, f'{row.Salary:.2f}', color='black', ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Average Salary by Position Group')\n",
    "plt.ylabel('Average Salary (L.E.)')\n",
    "plt.xlabel('Position Group')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824eaff-07a2-4d95-8464-de86008c33cb",
   "metadata": {},
   "source": [
    "## Distribute the Position & Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11901a-ecb0-4f55-865c-6b1a8fb5d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Define groups of keywords for the positions\n",
    "group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "\n",
    "# Function to categorize positions based on keywords\n",
    "def categorize_position(position, keywords):\n",
    "    if pd.isna(position):\n",
    "        return False\n",
    "    return any(keyword in position for keyword in keywords)\n",
    "\n",
    "# Filter and plot for each group\n",
    "def plot_salary_distribution(df, group_keywords, group_name):\n",
    "    filtered_df = df[df['Position'].apply(lambda x: categorize_position(x, group_keywords))]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Position', y='Salary', data=filtered_df, ci=None, palette='Set2', estimator=np.mean)\n",
    "        \n",
    "        # Add salary averages on the bars\n",
    "        for p in plt.gca().patches:\n",
    "            plt.gca().annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                               ha='center', va='center', fontsize=12, color='black', xytext=(0, 10), \n",
    "                               textcoords='offset points')\n",
    "        \n",
    "        plt.title(f'Average Salary for {group_name} Positions')\n",
    "        plt.ylabel('Average Salary (L.E.)')\n",
    "        plt.xlabel('Position')\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot charts for each group\n",
    "plot_salary_distribution(df, group_1, 'Director, Regional, Head, Manager, Executive')\n",
    "plot_salary_distribution(df, group_2, 'Associate, Assistant, Engineer')\n",
    "plot_salary_distribution(df, group_3, 'Developer, Accountant, Lawyer')\n",
    "plot_salary_distribution(df, group_4, 'Representative, Paralegal, Analyst')\n",
    "plot_salary_distribution(df, group_5, 'Intern, Specialist, Junior')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980574f-d96b-4333-8a19-6bebed52b740",
   "metadata": {},
   "source": [
    "## Another Try to distribute the Position & Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e352f4-be7f-4435-89c4-4ffb2821a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Define groups of keywords for the positions\n",
    "group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "\n",
    "# Function to categorize positions based on keywords\n",
    "def categorize_position(position, keywords):\n",
    "    if pd.isna(position):\n",
    "        return False\n",
    "    return any(keyword in position for keyword in keywords)\n",
    "\n",
    "# Function to filter and plot the top N positions by frequency\n",
    "def plot_salary_distribution(df, group_keywords, group_name, top_n=10):\n",
    "    filtered_df = df[df['Position'].apply(lambda x: categorize_position(x, group_keywords))]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        \n",
    "        # Keep only the top N positions by frequency\n",
    "        top_positions = filtered_df['Position'].value_counts().index[:top_n]\n",
    "        filtered_df = filtered_df[filtered_df['Position'].isin(top_positions)]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(x='Position', y='Salary', data=filtered_df, errorbar=None, palette='Set2', estimator=np.mean)\n",
    "        \n",
    "        # Add salary averages on the bars with staggered alignment (zig-zag)\n",
    "        for index, p in enumerate(ax.patches):\n",
    "            # Determine the vertical offset for the label (alternate up and down)\n",
    "            offset = 15 if index % 2 == 0 else -25  # Up for even, down for odd index\n",
    "            va = 'bottom' if index % 2 == 0 else 'top'  # Align text differently for up and down\n",
    "            ax.annotate(f'{p.get_height():.2f}', \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha='center', va=va, fontsize=10, color='black', \n",
    "                        xytext=(0, offset), textcoords='offset points')\n",
    "        \n",
    "        plt.title(f'Average Salary for {group_name} Positions')\n",
    "        plt.ylabel('Average Salary (L.E.)')\n",
    "        plt.xlabel('Position')\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot charts for each group with only the top 10 positions\n",
    "plot_salary_distribution(df, group_1, 'Director, Regional, Head, Manager, Executive')\n",
    "plot_salary_distribution(df, group_2, 'Associate, Assistant, Engineer')\n",
    "plot_salary_distribution(df, group_3, 'Developer, Accountant, Lawyer')\n",
    "plot_salary_distribution(df, group_4, 'Representative, Paralegal, Analyst')\n",
    "plot_salary_distribution(df, group_5, 'Intern, Specialist, Junior')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b523cf-66c5-490e-85a7-73e4c2d30897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Age and Salary\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Age', y='Salary', data=df)\n",
    "plt.title('Distribution between Age and Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edf6a3-5623-44d7-8da7-06dcab25ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Age and calculate the average salary for each age group\n",
    "age_salary = df.groupby('Age')['Salary'].mean().reset_index()\n",
    "\n",
    "# Plot the data using a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Age', y='Salary', data=age_salary, palette='Set2')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution between Age and Salary')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Average Salary (L.E.)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b22ea-41a7-4091-b006-ec75a02f6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Gender and Insurance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Insurance', hue='Gender', data=df)\n",
    "plt.title('Distribution between Gender and Insurance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708cbe8-29d6-4875-bee6-1f9947fd7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Charts for Categorical Features\n",
    "\n",
    "categorical_columns = [\n",
    "    'Gender', 'Marital_Status', 'Recruitment_channel', 'Education', \n",
    "    'Insurance', 'Over_Time', 'Business_Travel', 'Attrition'\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))\n",
    "    plt.title(f'Pie chart of {col}')\n",
    "    plt.ylabel('')  # Hide the y-label for a cleaner look\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672302d-a2ee-442d-9ae7-64d60db6c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Promotion rates across Departments\n",
    "promotion_rate = df.groupby('Department')['Promotion'].value_counts(normalize=True).unstack().fillna(0)\n",
    "promotion_rate.plot(kind='bar', stacked=True, figsize=(14, 8))\n",
    "plt.title('Promotion Rates Across Departments')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f3578-0550-4a92-a0e4-451441f4b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Promotion rates across Regions\n",
    "promotion_rate = df.groupby('Region')['Promotion'].value_counts(normalize=True).unstack().fillna(0)\n",
    "promotion_rate.plot(kind='bar', stacked=True, figsize=(14, 8))\n",
    "plt.title('Promotion Rates Across Regions')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d1edc0-7c04-4f71-98ac-5d527dfe10cb",
   "metadata": {},
   "source": [
    "## 📌 **_Attrition Analysis:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d05ae8-9681-4775-8f7b-f865015c68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Attrition across Regions\n",
    "attrition_rate = df.groupby('Region')['Attrition'].value_counts(normalize=True).unstack().fillna(0)\n",
    "attrition_rate.plot(kind='bar', stacked=True, figsize=(14, 8))\n",
    "plt.title('Attrition Rates Across Regions')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86c7d2-8bbc-4fae-b8a2-11fffd83bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each category of 'Attrition'\n",
    "custom_colors = {'Yes': 'red', 'No': 'Green'}\n",
    "\n",
    "# Create a histogram using plotly.express with custom colors\n",
    "fig = px.histogram(df, \n",
    "                   x='Business_Travel', \n",
    "                   color='Attrition', \n",
    "                   barmode='group',  # Set to 'group' to display bars side by side\n",
    "                   nbins=30, \n",
    "                   width=600, \n",
    "                   height=300,\n",
    "                   color_discrete_map=custom_colors)  # Use custom colors\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca24062-28c2-4d36-a9e1-ed9df5d9be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each category of 'Attrition'\n",
    "custom_colors = {'Yes': 'Black', 'No': 'Yellow'}\n",
    "\n",
    "# Create a histogram using plotly.express with custom colors\n",
    "fig = px.histogram(df, \n",
    "                   x='Over_Time', \n",
    "                   color='Attrition', \n",
    "                   barmode='group',  # Set to 'group' to display bars side by side\n",
    "                   nbins=30, \n",
    "                   width=600, \n",
    "                   height=300,\n",
    "                   color_discrete_map=custom_colors)  # Use custom colors\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddd58f-1d24-47d9-a41d-e8a01644eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each category of 'Attrition'\n",
    "custom_colors = {'Yes': 'Purple', 'No': 'Navy'}\n",
    "\n",
    "# Create a histogram using plotly.express with custom colors\n",
    "fig = px.histogram(df, \n",
    "                   x='Gender', \n",
    "                   color='Attrition', \n",
    "                   barmode='group',  # Set to 'group' to display bars side by side\n",
    "                   nbins=30, \n",
    "                   width=600, \n",
    "                   height=300,\n",
    "                   color_discrete_map=custom_colors)  # Use custom colors\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7280462-5455-406e-ac8a-640034cc8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each category of 'Attrition'\n",
    "custom_colors = {'Yes': 'Lime', 'No': 'Grey'}\n",
    "\n",
    "# Create a histogram using plotly.express with custom colors\n",
    "fig = px.histogram(df, \n",
    "                   x='Gender', \n",
    "                   color='Attrition', \n",
    "                   barmode='group',  # Set to 'group' to display bars side by side\n",
    "                   nbins=30, \n",
    "                   width=600, \n",
    "                   height=300,\n",
    "                   color_discrete_map=custom_colors)  # Use custom colors\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac3609-4a99-41b0-8ad9-cdbd5f18bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each category of 'Attrition'\n",
    "custom_colors = {'Yes': 'Lime', 'No': 'Grey'}\n",
    "\n",
    "# Create a histogram using plotly.express with custom colors\n",
    "fig = px.histogram(df, \n",
    "                   x='Position', \n",
    "                   color='Attrition', \n",
    "                   barmode='group',  # Set to 'group' to display bars side by side\n",
    "                   nbins=30, \n",
    "                   width=600, \n",
    "                   height=300,\n",
    "                   color_discrete_map=custom_colors)  # Use custom colors\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c91451-c9cf-43d7-9ed0-4f072566accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Gender and Attrition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Attrition', hue='Gender', data=df)\n",
    "plt.title('Distribution between Gender and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2341a45-0f73-41eb-a75b-f513aa4363ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Department and Attrition\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Department', hue='Attrition', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution between Department and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597e7b9-9849-44b2-a04f-c407564b2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Position and Attrition\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Position', hue='Attrition', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution between Position and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bd413-1f8d-4fca-bfb3-e32a216b39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Define groups of keywords for the positions\n",
    "group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "\n",
    "# Function to categorize positions based on keywords\n",
    "def categorize_position(position):\n",
    "    if any(keyword in position for keyword in group_1):\n",
    "        return 'Group 1 (Director, Regional, Head, Manager, Executive)'\n",
    "    elif any(keyword in position for keyword in group_2):\n",
    "        return 'Group 2 (Associate, Assistant, Engineer)'\n",
    "    elif any(keyword in position for keyword in group_3):\n",
    "        return 'Group 3 (Developer, Accountant, Lawyer)'\n",
    "    elif any(keyword in position for keyword in group_4):\n",
    "        return 'Group 4 (Representative, Paralegal, Analyst)'\n",
    "    elif any(keyword in position for keyword in group_5):\n",
    "        return 'Group 5 (Intern, Specialist, Junior)'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to categorize positions\n",
    "df['Position_Group'] = df['Position'].apply(categorize_position)\n",
    "\n",
    "# Filter out the 'Other' group to focus on the specified positions\n",
    "df_filtered = df[df['Position_Group'] != 'Other']\n",
    "\n",
    "# Plot the distribution of Attrition for each position group\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='Position_Group', hue='Attrition', data=df_filtered, palette='Set2')\n",
    "\n",
    "# Add counts on the bars\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha='center', va='center', fontsize=12, color='black', xytext=(0, 10), \n",
    "                       textcoords='offset points')\n",
    "\n",
    "plt.title('Distribution of Attrition by Title Level Group')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Title Level Group')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa03ad-946f-4e7f-9197-a996d11f45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Define groups of keywords for the positions\n",
    "group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "\n",
    "# Function to categorize positions based on keywords\n",
    "def categorize_position(position, keywords):\n",
    "    return any(keyword in position for keyword in keywords)\n",
    "\n",
    "# Filter and plot for each group\n",
    "def plot_attrition_distribution(df, group_keywords, group_name):\n",
    "    filtered_df = df[df['Position'].apply(lambda x: categorize_position(x, group_keywords))]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.countplot(x='Position', hue='Attrition', data=filtered_df, palette='Set2')\n",
    "        \n",
    "        # Add counts on the bars\n",
    "        for p in plt.gca().patches:\n",
    "            plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                               ha='center', va='center', fontsize=12, color='black', xytext=(0, 10), \n",
    "                               textcoords='offset points')\n",
    "        \n",
    "        plt.title(f'Distribution of Attrition for {group_name} Positions')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Position')\n",
    "        plt.xticks(rotation=30, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot charts for each group\n",
    "plot_attrition_distribution(df, group_1, 'Director, Regional, Head, Manager, Executive')\n",
    "plot_attrition_distribution(df, group_2, 'Associate, Assistant, Engineer')\n",
    "plot_attrition_distribution(df, group_3, 'Developer, Accountant, Lawyer')\n",
    "plot_attrition_distribution(df, group_4, 'Representative, Paralegal, Analyst')\n",
    "plot_attrition_distribution(df, group_5, 'Intern, Specialist, Junior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b714e-5025-4767-96e0-3cc44e83bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Marital Status and Attrition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Marital_Status', hue='Attrition', data=df)\n",
    "plt.title('Distribution between Marital Status and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411756cf-9c15-451a-8032-3fffe14e2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Salary and Attrition\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Attrition', y='Salary', data=df)\n",
    "plt.title('Distribution between Salary and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21dd00-1cec-46d0-aa55-83bb4760134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average salary for each attrition status\n",
    "avg_salary_by_attrition = df.groupby('Attrition')['Salary'].mean()\n",
    "\n",
    "# Plot the data as a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=avg_salary_by_attrition.index, y=avg_salary_by_attrition.values, palette='viridis')\n",
    "plt.title('Average Salary by Attrition Status')\n",
    "plt.xlabel('Attrition')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f4cad-12ff-4da1-ba5e-980a7edfc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Insurance and Attrition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Insurance', hue='Attrition', data=df)\n",
    "plt.title('Distribution between Insurance and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df8302-8e17-478e-b0e7-0ce74d8eddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Over Time and Attrition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Over_Time', hue='Attrition', data=df)\n",
    "plt.title('Distribution between Over Time and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cb2e6-d710-4033-afdf-1f905930fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Business Travel and Attrition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Business_Travel', hue='Attrition', data=df)\n",
    "plt.title('Distribution between Business Travel and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e43bf5-7292-48cd-ab18-0bbdc3ac2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Age and Attrition\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Attrition', y='Age', data=df)\n",
    "plt.title('Distribution between Age and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cdf84-1a5f-4ca8-8323-24ef5d1366ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution between Years In Company and Attrition\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Attrition', y='Years_In_Company', data=df)\n",
    "plt.title('Distribution between Years In Company and Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315361fd-3e12-4973-b946-e14db2aa8149",
   "metadata": {},
   "source": [
    "## 📝 **Conclusions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376110c-1312-4134-8e0c-565354decfe1",
   "metadata": {},
   "source": [
    "***\n",
    "- The workers with low **`MonthlyIncome`** are more likely to quit there jobs.\n",
    "\n",
    "- **`BusinessTravel`** : The people who **Travel alot** are more likely to quit than other employees.\n",
    "\n",
    "- **`Department`** : The employees in **`Operation, Finance & Procurment`** are more likely to quit than the employees on other departement.\n",
    "\n",
    "- **`JobRole`** : The employees in **`Associate`**, **`Assistant`**, **`Engineers`**, **`Specialist`**, **`Intern`** and **`Jumior`** are more likely to quit.\n",
    "\n",
    "- **`Posiiton`** : The employees in **`Procurment Specialist`**, **`Project Control Specialist`**, and **`Site Engineers`** are more likely to quit.\n",
    "\n",
    "- **`Gender`** : The **`Male`** are more likely to quit.\n",
    "\n",
    "- **`MaritalStatus`** : The workers who have **`Married & Divorced`** marital status are more likely to quit the **`Single`**, and **`Widowed`**.\n",
    "\n",
    "- **`OverTime`** : The workers who work more hours are likely to quit then others.\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f03ca-055e-46f3-85ff-f32400d3d8cd",
   "metadata": {},
   "source": [
    "# 📉 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8e209-3354-48c7-8721-ca08483855fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include in the correlation analysis\n",
    "features = [\n",
    "    'Salary', 'Position', 'Over_Time', 'Business_Travel', 'Years_In_Company',\n",
    "    'Attrition', 'Promotion', 'Years_of_trainings', 'Previous_Year_Rating',\n",
    "    'KPIs_met >80%', 'Awards_Won', 'Avg_Training_Score'\n",
    "]\n",
    "\n",
    "# Filter the dataframe to include only the relevant features\n",
    "df_corr = df[features].copy()  # Make an explicit copy of the DataFrame\n",
    "\n",
    "# Encoding categorical variables\n",
    "# Label Encoding for binary categorical variables\n",
    "binary_vars = ['Over_Time', 'Attrition']\n",
    "df_corr.loc[:, binary_vars] = df_corr[binary_vars].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Ordinal Encoding for 'Promotion'\n",
    "promotion_mapping = {'No': 0, 'Probably': 1, 'Yes': 2}\n",
    "df_corr.loc[:, 'Promotion'] = df_corr['Promotion'].map(promotion_mapping)\n",
    "\n",
    "# Ordinal Encoding for 'Business_Travel'\n",
    "travel_mapping = {'Non-Travel': 0, 'Rarely': 1, 'Frequently': 2}\n",
    "df_corr.loc[:, 'Business_Travel'] = df_corr['Business_Travel'].map(travel_mapping)\n",
    "\n",
    "# One-Hot Encoding for 'Position' since it can have multiple categories\n",
    "df_corr = pd.get_dummies(df_corr, columns=['Position'], drop_first=True)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b8e33-f84d-4cb7-8e85-104c44b6ce5b",
   "metadata": {},
   "source": [
    "## 📝 **Analysis of Correlation Matrix:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f467ab-2bc3-4013-891a-726f1d35e1c4",
   "metadata": {},
   "source": [
    "***\n",
    "- The correlation matrix we've performed, as displayed in the heatmap, shows the relationships between various features in the dataset. Here's an analysis of what the correlation matrix reveals:\n",
    "\n",
    "**`Key Observations:`**\n",
    "\n",
    "**`Salary:`**\n",
    "\n",
    "**`Low Correlation with Other Features:`** Salary appears to have a very low correlation with most other features, meaning it doesn't strongly linearly relate to the other variables in the dataset.\n",
    "Position: You can observe slight correlations between certain positions and salary, but overall, it's weak.\n",
    "\n",
    "**`Attrition:`**\n",
    "\n",
    "**`Minimal Correlation:`** The Attrition feature shows minimal correlation with other features such as Salary, Over_Time, and Business_Travel, suggesting that the reasons for attrition might not be strongly related to these factors alone.\n",
    "\n",
    "**`Business_Travel:`**\n",
    "\n",
    "**`Weak Correlation:`** The Business_Travel feature also shows weak correlation with other variables, including Salary and Position. This indicates that travel frequency doesn't significantly influence these variables.\n",
    "\n",
    "**`Years in Company:`**\n",
    "\n",
    "**`Higher Correlation with Promotion`** There may be a positive correlation between Years_In_Company and Promotion, suggesting that longer tenure could increase the likelihood of promotion.\n",
    "\n",
    "**`Position:`**\n",
    "\n",
    "**`Position Variables:`** The Position variables, which have been one-hot encoded, show very low correlations with each other, which is expected. Some positions might show slight correlation with salary, which would indicate that certain roles are associated with higher or lower pay.\n",
    "\n",
    "**`KPIs Met >80% and Previous Year Rating:`**\n",
    "\n",
    "**`Moderate Correlation:`** There might be some moderate correlation between these two features and Promotion or Awards_Won, suggesting that high performance and good ratings are likely to result in recognition and promotion.\n",
    "\n",
    "**`Awards Won:`**\n",
    "\n",
    "**`Low Correlation:`** Similar to other variables, this feature has weak correlations with most other features, though there might be some slight relationship with performance-related features like KPIs_met >80% and Previous_Year_Rating.\n",
    "\n",
    "***\n",
    "\n",
    "**`Interpretation:`**\n",
    "\n",
    "**`Weak Correlations:`** \n",
    "The overall weak correlations in the matrix suggest that many of the features in your dataset do not have a strong linear relationship with each other. This can sometimes indicate the need for more complex modeling approaches to capture non-linear relationships.\n",
    "\n",
    "**`Position and Salary:`** \n",
    "Positions might have some influence on salary, as seen from slight correlations, but the relationship is not very strong across the board.\n",
    "\n",
    "**`Attrition and Promotion:`**\n",
    "Both features seem to be influenced by multiple factors, but none of them dominate, indicating that employee behavior (attrition) and growth (promotion) are likely driven by a complex interplay of multiple factor.\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0324cb-bd5e-43e2-befb-2a5fe642d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89485def-c737-4cba-b2bf-50e8cf559cef",
   "metadata": {},
   "source": [
    "# 📌 **_Step 4- Data Preprocessing:_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9fe02-fa21-4f34-b9d5-7b0bd6eba4d0",
   "metadata": {},
   "source": [
    "<font size=\"4\">_Data preprocessing are crucial step before diving into modeling. This step ensure the dataset is clean, consistent, and ready for meaningful analysis._</font>\n",
    "\n",
    "<font size=\"4\">**_Data Preprocessing Steps_**</font>\n",
    "\n",
    "<font size=\"4\">**_a. Data Understanding_**</font>\n",
    "\n",
    "<font size=\"4\">*_The first step is to understand the data structure and identify any anomalies, such as missing values, duplicates, or incorrect formats._*</font>\n",
    "\n",
    "<font size=\"4\">**_Key actions:_**</font>\n",
    "\n",
    ">- **Load the dataset:** Import the dataset and check the first few rows to understand the data structure.\n",
    ">- **Examine data types:** Make sure that each column has the appropriate data type (e.g., numerical for salaries, categorical for gender, and dates for DOJ/DOB).\n",
    ">- **Summary statistics:** Calculate summary statistics (mean, median, min, max, standard deviation) for numerical columns and count frequencies for categorical columns.\n",
    "\n",
    "<font size=\"4\">**_b. Handling Missing Values_**</font>\n",
    "\n",
    "<font size=\"4\">_Missing data can skew analysis or predictions, so it's essential to handle it appropriately._</font>\n",
    "\n",
    ">- **Identify missing values:** Count missing values in each column.\n",
    ">- **Strategies for dealing with missing data:**\n",
    "    - Remove rows or columns if a significant portion of the data is missing.\n",
    "    - Impute missing values with statistical measures (e.g., mean for continuous variables or mode for categorical variables).\n",
    "    - Domain-specific imputation: For instance, if Previous_Year_Rating is missing, it might imply a new employee, so set it to 0 or a neutral rating.\n",
    "\n",
    "<font size=\"4\">**_c. Handling Duplicates_**</font>\n",
    "\n",
    "<font size=\"4\">_Duplicate records can inflate the dataset and lead to misleading results._</font>\n",
    "\n",
    ">- **Detect and remove duplicates:** Use pandas' duplicated() method to find and remove duplicates.\n",
    "\n",
    "<font size=\"4\">**_d. Data Formatting_**</font>\n",
    "\n",
    "<font size=\"4\">_Ensure all data is in the correct format (e.g., dates, categorical variables)._</font>\n",
    "\n",
    ">- **Dates:** Convert date columns (DOB, DOJ) to proper datetime format.\n",
    ">- **Categorical Variables:** Convert columns like Department, Position, and Marital_Status to categorical types.\n",
    "\n",
    "<font size=\"4\">**_e. Feature Engineering_**</font>\n",
    "\n",
    "<font size=\"4\">_Feature engineering is the process of creating new features or modifying existing ones to enhance the dataset._</font>\n",
    "\n",
    ">- **Years In Company:** Calculate years in company by subtracting the date of joining (DOJ) from the current date.\n",
    ">- **Years of Experience:** Calculate years of Experience by subtracting 21 years from the difference between the (DOJ) and current date.\n",
    ">- **Age:** Calculate employee age based on their date of birth (DOB).\n",
    "\n",
    "<font size=\"4\">**_f. Dealing with Outliers_**</font>\n",
    "\n",
    "<font size=\"4\">_Outliers can distort analysis and models. Common techniques to handle outliers include:_</font>\n",
    "\n",
    ">- **Removing outliers based on domain knowledge** (e.g., extremely high or low salaries).\n",
    ">- **Transforming data** (e.g., log transformation) to reduce the impact of outliers.\n",
    "\n",
    "<font size=\"4\">**_g. Encoding Categorical Variables_**</font>\n",
    "\n",
    "<font size=\"4\">_Many machine learning models require numerical input, so categorical variables need to be encoded. Common encoding techniques include:_</font>\n",
    "\n",
    ">- **Label encoding:** For ordinal data (e.g., Years_of_Experience).\n",
    ">- **One-hot encoding:** For nominal data (e.g., Department, Gender).\n",
    "\n",
    "<font size=\"4\">**_h. Feature_**</font>\n",
    "\n",
    "<font size=\"4\">_Some machine learning algorithms perform better when features are on a similar scale. Normalization or standardization can be applied to numerical features (e.g., salary, years of experience)._</font>\n",
    "\n",
    "<font size=\"4\">**_3. Preprocessing Checklist_**</font>\n",
    "\n",
    ">- **Handling missing values:** Imputed missing data or removed records with missing fields.\n",
    ">- **Removing duplicates:** Checked and removed duplicate rows.\n",
    ">- **Data formatting:** Ensured proper formats for dates, numerical, and categorical columns.\n",
    ">- **Feature engineering:** Created new variables such as Age and Years In Company.\n",
    ">- **Handling outliers:** Detected and managed outliers in salary and other variables.\n",
    ">- **Encoding categorical variables:** Converted categorical variables into numerical format for model input.\n",
    ">- **Scaling numerical features:** Applied scaling to numerical features like salary and years of experience.\n",
    "\n",
    "<font size=\"4\">**_Conclusion:_**</font>\n",
    "\n",
    "<font size=\"4\">_Data mining and preprocessing are essential to prepare the dataset for analysis and model building. After preprocessing, you'll have a clean, consistent dataset that is ready for deeper analysis, machine learning, or predictive modeling, helping you answer business questions and derive actionable insights._</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74754281-7875-479c-8053-265e785737f5",
   "metadata": {},
   "source": [
    "<font size=\"5\" font color='green'> 📌 **_Data Preprocessing_**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f9faa-9155-4af0-a54f-c858ee16262d",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**`Step 1: Data Cleaning:`**\n",
    "\n",
    "**`a) Noisy Data:`**\n",
    "\n",
    ">- **_Identification and Removal:_**\n",
    "   We'll detect and handle outliers using methods like the Z-score or IQR\n",
    "   (Interquartile Range), then remove or smooth any noisy data.   \n",
    "\n",
    "**`b) Binning Method:`**\n",
    "\n",
    ">- **_Binning:_**\n",
    "   We can group continuous variables like age or salary into bins (e.g., age ranges\n",
    "   or salary ranges) to reduce the impact of minor observation errors.\n",
    "\n",
    "**`c) Clustering:`**\n",
    "\n",
    ">- **_Clustering for Noise Reduction:_**\n",
    "   Apply clustering algorithms (e.g., K-Means) to identify and handle noise by\n",
    "   grouping similar data points together.\n",
    "\n",
    "\n",
    "**`Step 2: Data Transformation`**\n",
    "\n",
    "**`a) Normalization:`**\n",
    "\n",
    ">- **_Normalization:_**\n",
    "   Normalize & scale the numerical features to a common scale, usually between 0 and 1, using Min-\n",
    "   Max scaling or Z-score standardization.\n",
    "\n",
    "**`b) Attribute Selection:`**\n",
    "\n",
    ">- **_Feature Selection:_**\n",
    "   Select the most relevant features based on correlation or feature importance.\n",
    "\n",
    "**`c) Discretization:`**\n",
    "\n",
    ">- **_Discretization:_**\n",
    "   Convert continuous variables into categorical bins if needed.\n",
    "\n",
    "**`d) Concept Hierarchy Generation:`**\n",
    "\n",
    ">- **_Hierarchy Generation:_**\n",
    "   Create a hierarchy of categorical features (e.g., position levels).\n",
    "\n",
    "**`Step 3: Data Reduction`**\n",
    "\n",
    "**`a) Feature Selection:`**\n",
    "\n",
    ">- **_Feature Selection:_**\n",
    "   Use techniques like Recursive Feature Elimination (RFE) or Lasso Regression to\n",
    "   select the most important features.\n",
    "  \n",
    "**`b) Feature Extraction:`**\n",
    "\n",
    ">- **_PCA:_**\n",
    "   Use Principal Component Analysis (PCA) for dimensionality reduction while\n",
    "   retaining as much variance as possible to extract new features from the existing ones.\n",
    "\n",
    "**`c) Sampling:`**\n",
    "\n",
    ">- **_Sampling:_**\n",
    "   If the dataset is too large, we can sample a portion of it for faster processing.\n",
    "\n",
    "**`d) Clustering:`**\n",
    "\n",
    ">- **_Clustering:_**\n",
    "   Use clustering to group similar data points and reduce dimensionality.\n",
    "\n",
    "**`e) Imbalanced Data:`**\n",
    "\n",
    ">- **_Imbalance Handling:_**\n",
    "   If the target variable (like attrition) is imbalanced, we'll apply techniques like\n",
    "   SMOTE or undersampling.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff86e3-4a29-4b61-9a18-bff5fa95a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 1- Data Cleaning: \n",
    "# ==============================================\n",
    "#Load the dataset\n",
    "df = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "\n",
    "# Handle Missing Values using Median Imputation\n",
    "# df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Identify and cap outliers based on the z-score (removing extreme outliers)\n",
    "df = df[(np.abs(stats.zscore(df.select_dtypes(include=[np.number]))) < 20).all(axis=1)]\n",
    "\n",
    "# ==============================================\n",
    "# 2- Binning continuous variables for Age, Years_of_Experience, Years_In_Company, and Gross_Salary\n",
    "# ==============================================\n",
    "age_bins = [20, 30, 40, 50, 60, 70]\n",
    "age_labels = ['20-30', '30-40', '40-50', '50-60', '60+']\n",
    "df['Age_Binned'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "\n",
    "# Binning for Years_of_Experience\n",
    "experience_bins = [0, 10, 20, 30, 40, 50]\n",
    "experience_labels = ['0-10', '10-20', '20-30', '30-40', '40+']\n",
    "df['Experience_Binned'] = pd.cut(df['Years_of_Experience'], bins=experience_bins, labels=experience_labels)\n",
    "\n",
    "# Binning for Years_In_Company\n",
    "company_bins = [0, 10, 20, 30, 40]\n",
    "company_labels = ['0-10', '10-20', '20-30', '30-40']\n",
    "df['Company_Binned'] = pd.cut(df['Years_In_Company'], bins=company_bins, labels=company_labels)\n",
    "\n",
    "# Binning for Gross_Salary\n",
    "salary_bins = [0, 50000, 100000, 200000, 500000, 1000000]\n",
    "salary_labels = ['0-50K', '50K-100K', '100K-200K', '200K-500K', '500K-1M']\n",
    "df['Gross_Salary_Binned'] = pd.cut(df['Gross_Salary'], bins=salary_bins, labels=salary_labels)\n",
    "\n",
    "# ==============================================\n",
    "# 3- Normalization and Scaling will be applied in the pipeline to avoid data leakage\n",
    "# ==============================================\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Clustering - Apply after Scaling\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Exclude Employee_ID from scaling and clustering\n",
    "employee_ids = df['Employee_id']  # Save Employee_ID for later use\n",
    "\n",
    "# Select only numeric columns, excluding Employee_ID\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols = numeric_cols.drop('Employee_id')  # Exclude Employee_ID\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#df_scaled = pd.DataFrame(scaler.fit_transform(df.select_dtypes(include=[np.number])), \n",
    "#                         columns=df.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)\n",
    "\n",
    "# Add Employee_ID back to the scaled DataFrame (without scaling it)\n",
    "df_scaled['Employee_id'] = employee_ids\n",
    "\n",
    "# Save the scaled DataFrame\n",
    "df_scaled.to_csv('8-scaled_dataset.csv', index=False)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df_scaled['Cluster'] = kmeans.fit_predict(df_scaled[numeric_cols])  # Only use numeric columns for clustering\n",
    "\n",
    "# Save KMeans model for future use\n",
    "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "\n",
    "# Save the scaled DataFrame with cluster column\n",
    "df_scaled.to_csv('9-clustered_dataset.csv', index=False)\n",
    "\n",
    "print(f\"Final row count after scaling and clustering: {df_scaled.shape[0]}\")\n",
    "\n",
    "# ===============================================\n",
    "# Data Splitting for Attrition and Promotion\n",
    "# ===============================================\n",
    "\n",
    "# Define features (X) and target variables (y) for Attrition and Promotion\n",
    "X = df.drop(columns=['Attrition', 'Promotion'], errors='ignore')\n",
    "y_attrition = df['Attrition']\n",
    "y_promotion = df['Promotion']\n",
    "\n",
    "# Split the data for Attrition prediction\n",
    "X_train_attr, X_test_attr, y_train_attr, y_test_attr = train_test_split(X, y_attrition, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data for Promotion prediction\n",
    "X_train_prom, X_test_prom, y_train_prom, y_test_prom = train_test_split(X, y_promotion, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce2d17-962e-4457-b8a6-b958000a4812",
   "metadata": {},
   "source": [
    "# 📌 **_Step 5- Model Data:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131f246-16d9-45cb-be57-bf7a90563fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Model Pipelines for Multiple Classifiers\n",
    "# ===============================================\n",
    "# Preprocessor pipeline for scaling and encoding\n",
    "categorical_cols = ['Department', 'Gender', 'Marital_Status', 'Over_Time', 'Business_Travel']\n",
    "numeric_cols = ['Years_of_Experience', 'Age', 'HRA', 'DA', 'PF', 'Gross_Salary', 'KPIs_met >80%', 'Awards_Won', 'Avg_Training_Score']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Classifiers to evaluate\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'XGBoost': XGBClassifier(eval_metric='mlogloss'),\n",
    "    'SVC': SVC(),\n",
    "    'DecisionTree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# ===============================================\n",
    "# Encode Target Variables\n",
    "# ===============================================\n",
    "# Initialize label encoder\n",
    "label_encoder_attrition = LabelEncoder()\n",
    "label_encoder_promotion = LabelEncoder()\n",
    "\n",
    "# Encode Attrition: 'Yes' -> 1, 'No' -> 0\n",
    "y_train_attr_encoded = label_encoder_attrition.fit_transform(y_train_attr)\n",
    "y_test_attr_encoded = label_encoder_attrition.transform(y_test_attr)\n",
    "\n",
    "# Encode Promotion: 'Yes' -> 1, 'No' -> 0, 'Probably' -> 2 (multiclass problem)\n",
    "y_train_prom_encoded = label_encoder_promotion.fit_transform(y_train_prom)\n",
    "y_test_prom_encoded = label_encoder_promotion.transform(y_test_prom)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# Evaluate Classifiers for Both Attrition and Promotion\n",
    "# ===============================================\n",
    "def evaluate_classifiers(X_train, X_test, y_train, y_test, classifiers, label, is_multiclass=False):\n",
    "    best_classifier = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for name, model in classifiers.items():\n",
    "        for random_state in [42, 101, 123]:  # Test multiple random states\n",
    "            model.set_params(random_state=random_state)\n",
    "            pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Save the trained pipeline\n",
    "            joblib.dump(pipeline, f'{name}_{label}_state_{random_state}.pkl')\n",
    "            \n",
    "            # Predict on test data\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "            print(f\"{label} - {name} (Random State: {random_state}) Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "            # Print Classification Report and Confusion Matrix\n",
    "            if is_multiclass:\n",
    "                print(f\"Classification Report for {label} - {name} (Random State: {random_state}):\")\n",
    "                print(classification_report(y_test, y_pred, target_names=['No', 'Yes', 'Probably'], zero_division=1))\n",
    "                print(f\"Confusion Matrix for {label} - {name} (Random State: {random_state}):\")\n",
    "                print(confusion_matrix(y_test, y_pred))\n",
    "            else:\n",
    "                print(f\"Classification Report for {label} - {name} (Random State: {random_state}):\")\n",
    "                print(classification_report(y_test, y_pred, target_names=['No', 'Yes'], zero_division=1))\n",
    "                print(f\"Confusion Matrix for {label} - {name} (Random State: {random_state}):\")\n",
    "                print(confusion_matrix(y_test, y_pred))\n",
    "                \n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Store the best classifier\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_classifier = (name, random_state)\n",
    "    \n",
    "    print(f\"\\nBest Classifier for {label}: {best_classifier[0]} with an accuracy of {best_accuracy:.2f}% (Random State: {best_classifier[1]})\\n\")\n",
    "    return best_classifier\n",
    "\n",
    "# Evaluate for Attrition (Binary Classification)\n",
    "best_attr_classifier = evaluate_classifiers(X_train_attr, X_test_attr, y_train_attr_encoded, y_test_attr_encoded, classifiers, 'Attrition')\n",
    "\n",
    "# Evaluate for Promotion (Multiclass Classification)\n",
    "best_prom_classifier = evaluate_classifiers(X_train_prom, X_test_prom, y_train_prom_encoded, y_test_prom_encoded, classifiers, 'Promotion', is_multiclass=True)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# Save the Best Classifiers for Streamlit\n",
    "# ===============================================\n",
    "best_attr_model_name, best_attr_state = best_attr_classifier\n",
    "best_prom_model_name, best_prom_state = best_prom_classifier\n",
    "\n",
    "# Load and save the best attrition model\n",
    "best_attr_pipeline = joblib.load(f'{best_attr_model_name}_Attrition_state_{best_attr_state}.pkl')\n",
    "joblib.dump(best_attr_pipeline, 'best_attrition_model.pkl')\n",
    "\n",
    "# Load and save the best promotion model\n",
    "best_prom_pipeline = joblib.load(f'{best_prom_model_name}_Promotion_state_{best_prom_state}.pkl')\n",
    "joblib.dump(best_prom_pipeline, 'best_promotion_model.pkl')\n",
    "\n",
    "print(\"Best models saved successfully for Streamlit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d3826-ca6f-44cc-b238-9476d6c9ce71",
   "metadata": {},
   "source": [
    "# 📌 **_Step 6- Interpret Results:_**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cedb7e6-d891-48cb-b881-721cc7942726",
   "metadata": {},
   "source": [
    "<font size=\"4\">🚨 **_We have completed Evaluatation of Model Performance: Precision, Recall, F1, and Confusion Matrix as following:_**</font>\n",
    ">- Best Classifier for Attrition: XGBoost with an accuracy of 86.15%\n",
    ">- Best Classifier for Promotion: XGBoost with an accuracy of 84.63%\n",
    "\n",
    "<font size=\"4\">🛠 **_We are going to perform the following:_**</font>\n",
    ">- a) Feature Importance (for Tree-Based Models like RandomForest, XGBoost)\n",
    ">- b.1) SHAP (SHapley Additive exPlanations) for Model Interpretation\n",
    ">- b.2) OR LIME (Local Interpretable Model-agnostic Explanations)\n",
    ">- c) Cross-Validation Results for Bias-Variance Tradeoff\n",
    ">- d) Business Insights and Visualization\n",
    "\n",
    "\n",
    "<font size=\"4\">📑 **_Summary:_**</font>\n",
    ">- Evaluate model performance using metrics like precision, recall, and confusion matrix.\n",
    ">- Analyze feature importance for models with XGBoost.\n",
    ">- Use SHAP or LIME for detailed prediction explanations.\n",
    ">- Visualize insights to support your findings and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d45ae-51b6-4a98-a47e-eec1dbb25622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Model Interpretation with SHAP\n",
    "# ===============================================\n",
    "# SHAP for Attrition\n",
    "\n",
    "# Preprocess the data using the pipeline's preprocessor (transform data to numeric)\n",
    "X_train_attr_preprocessed = best_attr_pipeline.named_steps['preprocessor'].transform(X_train_attr)\n",
    "X_test_attr_preprocessed = best_attr_pipeline.named_steps['preprocessor'].transform(X_test_attr)\n",
    "\n",
    "# Initialize SHAP explainer for Attrition using the classifier\n",
    "explainer_attr = shap.Explainer(best_attr_pipeline.named_steps['classifier'], X_train_attr_preprocessed)\n",
    "\n",
    "# Disable additivity check due to small floating point differences\n",
    "shap_values_attr = explainer_attr(X_test_attr_preprocessed, check_additivity=False)\n",
    "\n",
    "# SHAP Summary Plot for Attrition\n",
    "shap.summary_plot(shap_values_attr, X_test_attr_preprocessed)\n",
    "\n",
    "# Save SHAP explainer for Attrition\n",
    "joblib.dump(explainer_attr, 'shap_explainer_attr.pkl')\n",
    "\n",
    "# ===============================================\n",
    "# SHAP for Promotion\n",
    "\n",
    "# Preprocess the data using the pipeline's preprocessor (transform data to numeric)\n",
    "X_train_prom_preprocessed = best_prom_pipeline.named_steps['preprocessor'].transform(X_train_prom)\n",
    "X_test_prom_preprocessed = best_prom_pipeline.named_steps['preprocessor'].transform(X_test_prom)\n",
    "\n",
    "# Initialize SHAP explainer for Promotion using the classifier\n",
    "explainer_prom = shap.Explainer(best_prom_pipeline.named_steps['classifier'], X_train_prom_preprocessed)\n",
    "\n",
    "# Disable additivity check due to small floating point differences\n",
    "shap_values_prom = explainer_prom(X_test_prom_preprocessed, check_additivity=False)\n",
    "\n",
    "# SHAP Summary Plot for Promotion\n",
    "shap.summary_plot(shap_values_prom, X_test_prom_preprocessed)\n",
    "\n",
    "# Save SHAP explainer for Promotion\n",
    "joblib.dump(explainer_prom, 'shap_explainer_prom.pkl')\n",
    "\n",
    "print(\"SHAP explainers saved for Streamlit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30899998-05e2-43f1-ba7a-0402b4c409cd",
   "metadata": {},
   "source": [
    "# ♨ **_Step 7- Deployment:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258952a9-bde0-4d6a-8131-be65583ed01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile HR_Prediction_System.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "eda_dataset = pd.read_csv('7-Ready_Emp_Dataset.csv')\n",
    "# modeling_dataset = pd.read_csv('10-Ready_Data_Processed.csv')\n",
    "\n",
    "# Define pages\n",
    "pages = {\n",
    "    \"Home Page\": \"home\",\n",
    "    \"Statistics\": \"statistics\",\n",
    "    \"Prediction\": \"prediction\",\n",
    "    # Add other page names here if needed\n",
    "}\n",
    "\n",
    "# Sidebar for page navigation\n",
    "st.sidebar.title(\"Navigation\")\n",
    "selected_page = st.sidebar.selectbox(\"Select a page\", list(pages.keys()))\n",
    "\n",
    "# Page 1: Home Page\n",
    "if selected_page == \"Home Page\":\n",
    "\n",
    "    # Define image URLs or upload paths\n",
    "    top_right_image_path = os.path.join('epsilon-AI-logo-White2.jpg')\n",
    "    bottom_left_image_path = os.path.join('pic1.jpg')\n",
    "    bottom_image_path = os.path.join('machine-learning-process-flow.jpg')\n",
    "\n",
    "    # Display the top-right image (local image)\n",
    "    st.image(top_right_image_path, width=300, caption=\"Final Project AUG 2024\")\n",
    "    \n",
    "    st.title(\"Home Page\")\n",
    "    st.write(\"Welcome to the Home Page of the Employee Attrition & Promotion Prediction App.\")\n",
    "    st.write(\"Use the sidebar to navigate through different sections.\")\n",
    "    \n",
    "    # Add a custom CSS to position the top-right image and style the page\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .top-right {\n",
    "        position: absolute;\n",
    "        top: 10px;\n",
    "        right: 50px;\n",
    "        width: 300px;\n",
    "        height: auto;\n",
    "    }\n",
    "    .footer {\n",
    "        position: fixed;\n",
    "        left: 0;\n",
    "        bottom: 0;\n",
    "        width: 100%;\n",
    "        background-color: #fc8a15;\n",
    "        color: #141010;\n",
    "        text-align: center;\n",
    "        padding: 0px;  /* Reduced padding to shorten the footer */\n",
    "        margin-bottom: 0;\n",
    "        font-size: 14px;\n",
    "        font-weight: bold;  /* Makes text bold */\n",
    "        font-style: italic;  /* Makes text italic */\n",
    "    }\n",
    "    .content {\n",
    "        padding-top: 10px;\n",
    "    }\n",
    "    .half-half {\n",
    "        display: flex;\n",
    "        justify-content: space-between;\n",
    "    }\n",
    "    .half {\n",
    "        width: 48%;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    # Content for the middle section (Two vertical halves)\n",
    "    st.markdown(\"<div class='content'>\", unsafe_allow_html=True)\n",
    "    \n",
    "    # Create two columns for text and image in the bottom half\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"\"\"\n",
    "            <h3>About the System</h3>\n",
    "            <p>\n",
    "            This system allows you to predict whether an employee is likely to leave the company or be promoted based on various input factors.\n",
    "            </p>\n",
    "            <ul>\n",
    "                <li>Enter employee details like department, position, salary, etc.</li>\n",
    "                <li>Get the probability of attrition or promotion based on machine learning models.</li>\n",
    "            </ul>\n",
    "            <p>\n",
    "            Our predictive system uses state-of-the-art algorithms to help organizations better manage their workforce and make informed decisions.\n",
    "            </p>\n",
    "            <p>\n",
    "            With an intuitive interface, managers can easily input data and receive real-time predictions on employee behavior. \n",
    "            It is designed to integrate seamlessly into existing HR workflows, providing key insights without disrupting daily operations.\n",
    "            <br>\n",
    "            <br>\n",
    "            The system's ML models are continuously updated to reflect the latest workforce trends, ensuring that predictions \n",
    "            remain relevant and adaptive to changing business environments.\n",
    "            </p>     \n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "        # Display the image in the right half (local image)\n",
    "        st.image(bottom_image_path, width=300, caption=\"Machine Learing Process\", use_column_width=True)\n",
    "    \n",
    "    with col2:\n",
    "        # Display the image in the right half (local image)\n",
    "        st.image(bottom_left_image_path, width=300, caption=\"Predictive System Overview\", use_column_width=True)\n",
    "\n",
    "        st.markdown(\"\"\"\n",
    "            <h5>Objective of the project</h5>\n",
    "            <p>\n",
    "            The main goal of this project is to derive insights from the employee dataset that can help the company improve its workforce management.\n",
    "            </p>\n",
    "            <p>\n",
    "            Attrition Prediction: What factors are driving employee attrition? Can we predict whether an employee will leave the organization based on \n",
    "            features like salary, years of experience, or KPIs?\n",
    "            <br>\n",
    "            <br>\n",
    "            Promotion Factors: What are the key factors contributing to employee promotion? How do training, education, \n",
    "            and performance metrics affect promotion rates?\n",
    "            </p>\n",
    "            Prepared by:\n",
    "            OMAR HARB\n",
    "            <br>\n",
    "            Business Development Manager\n",
    "            <br>\n",
    "            Egypt, Cairo, Heliopolis\n",
    "            <br>\n",
    "            All rights reserved © Omar ElFarouk Ahmed\n",
    "            </p>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "        \n",
    "    # Close content section\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    # Footer section\n",
    "    st.markdown(\"\"\"\n",
    "        <div class=\"footer\">\n",
    "            <p><strong><em>© 2024 Employee Prediction System | Powered by OMAR HARB AI SOLUTIONS</em></strong></p>\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "\n",
    "# Page 2: Statistics\n",
    "elif selected_page == \"Statistics\":\n",
    "    st.title(\"Statistics\")\n",
    "    st.write(\"Explore various distributions and statistical insights in this section.\")\n",
    "    st.write(eda_dataset.head())\n",
    "    \n",
    "    # Age Distribution\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(eda_dataset['Age'], kde=True)\n",
    "    plt.title('Age Distribution')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Salary Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Salary', data=eda_dataset)\n",
    "    plt.title('Salary Distribution')\n",
    "    plt.xlabel('Salary')\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Correlation Matrix for Numeric Features\n",
    "    numeric_columns = eda_dataset.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numeric_columns.corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix for Numeric Features')\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Salary Distribution Across Departments\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Department', y='Salary', data=eda_dataset)\n",
    "    plt.title('Salary Distribution Across Departments')\n",
    "    plt.xlabel('Department')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.xticks(rotation=45)\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Salary Distribution Across Regions\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Region', y='Salary', data=eda_dataset)\n",
    "    plt.title('Salary Distribution Across Regions')\n",
    "    plt.xlabel('Region')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.xticks(rotation=45)\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Salary and Gender Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Gender', y='Salary', data=eda_dataset)\n",
    "    plt.title('Distribution between Salary and Gender')\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Salary Distribution Across Positions\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Position', y='Salary', data=eda_dataset)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Distribution between Salary and Position')\n",
    "    st.pyplot(plt)\n",
    "\n",
    "    # Distribution between Gender and Marital Status\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Marital_Status', hue='Gender', data=eda_dataset)\n",
    "    plt.title('Distribution between Gender and Marital Status')\n",
    "    st.pyplot(plt)\n",
    "    \n",
    "    # Average Salary by Position Group\n",
    "    def categorize_position(position):\n",
    "        group_1 = ['Director', 'Regional', 'Head', 'Manager', 'Executive']\n",
    "        group_2 = ['Associate', 'Assistant', 'Engineer']\n",
    "        group_3 = ['Developer', 'Accountant', 'Lawyer']\n",
    "        group_4 = ['Representative', 'Paralegal', 'Analyst']\n",
    "        group_5 = ['Intern', 'Specialist', 'Junior']\n",
    "        \n",
    "        if any(keyword in position for keyword in group_1):\n",
    "            return 'Group 1 (Director, Regional, Head, Manager, Executive)'\n",
    "        elif any(keyword in position for keyword in group_2):\n",
    "            return 'Group 2 (Associate, Assistant, Engineer)'\n",
    "        elif any(keyword in position for keyword in group_3):\n",
    "            return 'Group 3 (Developer, Accountant, Lawyer)'\n",
    "        elif any(keyword in position for keyword in group_4):\n",
    "            return 'Group 4 (Representative, Paralegal, Analyst)'\n",
    "        elif any(keyword in position for keyword in group_5):\n",
    "            return 'Group 5 (Intern, Specialist, Junior)'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    eda_dataset['Position_Group'] = eda_dataset['Position'].apply(categorize_position)\n",
    "    filtered_eda_dataset = eda_dataset[eda_dataset['Position_Group'] != 'Other']\n",
    "    \n",
    "    avg_salary = filtered_eda_dataset.groupby('Position_Group')['Salary'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Position_Group', y='Salary', data=avg_salary, palette='Set2')\n",
    "    plt.title('Average Salary by Position Group')\n",
    "    plt.xlabel('Position Group')\n",
    "    plt.ylabel('Average Salary')\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    st.pyplot(plt)\n",
    "\n",
    "\n",
    "# Page 3: Prediction\n",
    "elif selected_page == \"Prediction\":\n",
    "    # Load the saved models for Attrition and Promotion\n",
    "    attrition_model = joblib.load('best_attrition_model.pkl')\n",
    "    promotion_model = joblib.load('best_promotion_model.pkl')\n",
    "    \n",
    "    # Title & Sub-header of the page\n",
    "    st.title(\"Employee Attrition & Promotion Prediction\")\n",
    "    st.write(\"Make a prediction for employee attrition or promotion.\")\n",
    "    st.subheader(\"Enter Employee Information to Predict Attrition and Promotion\")\n",
    "\n",
    "    # Input fields for user data\n",
    "    # Department\n",
    "    department = st.selectbox(\"Select Department\", [\n",
    "    'Finance', 'Analytics', 'Operations', 'IT', 'Human Resources', 'Legal', 'Marketing',\n",
    "    'Sales', 'Procurement', 'Project Control', 'Quality Control/Quality Assurance'])\n",
    "\n",
    "    # Position\n",
    "    position = st.selectbox(\"Select Position\", [\n",
    "    'Account Director', 'Account Associate', 'Accountant', 'Analytics Director', 'Analytics Manager',\n",
    "    'Construction Engineer', 'Data Analyst', 'Developer', 'ERP Head', 'Finance Director', 'Finance Manager',\n",
    "    'HR Assistant', 'HR Director', 'HR Manager', 'HR Specialist', 'HR Executive', 'IT Director', 'IT Manager',\n",
    "    'Junior Accountant', 'Junior Analyst', 'Junior Developer', 'Lawyer', 'Legal Manager', 'Legal Assistant',\n",
    "    'Legal Director', 'Marketing Assistant', 'Marketing Director', 'Marketing Manager', 'Marketing Specialist',\n",
    "    'MEP Engineer', 'National Account Head', 'National Marketing Manager', 'National Sales Manager', \n",
    "    'Operations Director', 'Paralegal', 'Procurement Assistant', 'Procurement Director', 'Procurement Manager',\n",
    "    'Procurement Specialist', 'Project Control Assistant', 'Project Control Director', 'Project Control Manager',\n",
    "    'Project Control Specialist', 'Project Manager', 'QA Director', 'QA Lead', 'QA Manager', 'QA Specialist',\n",
    "    'QA Engineer II', 'QC Inspector', 'QC Manager', 'Recruitment Manager', 'Regional Account Head',\n",
    "    'Regional Marketing Manager', 'Regional Sales Manager', 'Sales Director', 'Sales Executive', 'Sales Representative',\n",
    "    'Senior Account Executive', 'Senior Accountant', 'Senior Developer', 'Senior Executive', 'Senior HR', 'Senior Analyst',\n",
    "    'Senior Marketing Executive', 'Site Engineer', 'Software Engineer III', 'Technical Office Engineer', 'Technical Lead',\n",
    "    'Account Executive', 'Account Representative', 'Accounts Intern', 'Business Development Representative', \n",
    "    'HR Associate', 'HR Representative', 'HR Intern', 'Marketing Associate', 'Marketing Representative', \n",
    "    'Marketing Intern', 'Marketing Development Representative', 'Sales Development Representative',\n",
    "    'Software Engineer II', 'Software Engineer I', 'QA Engineer I'])\n",
    "\n",
    "    # Education\n",
    "    education = st.selectbox(\"Select Education Level\", ['Bachelor', 'Masters & above'])\n",
    "    \n",
    "    # Region\n",
    "    region = st.slider('Region', 1, 34, 1)\n",
    "    \n",
    "    # Recruitment Channel\n",
    "    recruitment_channel = st.selectbox(\"Recruitment Channel\", ['Sourcing', 'Referred', 'Other'])\n",
    "    \n",
    "    # Years of Trainings\n",
    "    years_of_trainings = st.number_input('Years of Trainings', 0, 50, 1)\n",
    "\n",
    "    # Years of Experience\n",
    "    years_of_experience = st.slider('Years of Experience', 0, 50, 1)\n",
    "    \n",
    "    # Previous Year Rating\n",
    "    previous_year_rating = st.number_input('Previous Year Rating', 1, 5, 3)\n",
    "    \n",
    "    # KPIs met >80%\n",
    "    kpis_met = st.selectbox(\"KPIs Met >80%\", [0, 1])\n",
    "    \n",
    "    # Awards Won\n",
    "    awards_won = st.selectbox(\"Awards Won\", [0, 1])\n",
    "    \n",
    "    # Average Training Score\n",
    "    avg_training_score = st.number_input(\"Avg Training Score\", 30, 100, 70)\n",
    "    \n",
    "    # Gender\n",
    "    gender = st.selectbox(\"Gender\", ['Male', 'Female', 'Other'])\n",
    "    \n",
    "    # Marital Status\n",
    "    marital_status = st.selectbox(\"Marital Status\", ['Married', 'Divorced', 'Widowed', 'Single'])\n",
    "    \n",
    "    # Dependents\n",
    "    dependents = st.number_input('Dependents', 0, 10, 0)\n",
    "\n",
    "    # Age\n",
    "    age = st.slider('Age', 21, 50, 65)\n",
    "    \n",
    "    # Basic Salary\n",
    "    salary = st.number_input('Salary', 3000, 1000000, 50000)\n",
    "\n",
    "    # HRA\n",
    "    #HRA = st.number_input('House Rent Allowance', 3000, 1000000, 50000)\n",
    "\n",
    "    # DA\n",
    "    #DA = st.number_input('Dearness Allowance', 3000, 1000000, 50000)\n",
    "\n",
    "    # PF\n",
    "    #PF = st.number_input('Provident Fund', 3000, 1000000, 50000)\n",
    "    \n",
    "    # Gross Salary\n",
    "    #gross_salary = st.number_input('Gross Salary', 3000, 1000000, 50000)\n",
    "    \n",
    "    # Insurance\n",
    "    insurance = st.selectbox(\"Insurance Type\", ['Medical', 'Life', 'Both'])\n",
    "    \n",
    "    # Over Time\n",
    "    over_time = st.selectbox(\"Over Time\", ['Yes', 'No'])\n",
    "    \n",
    "    # Business Travel\n",
    "    business_travel = st.selectbox(\"Business Travel Frequency\", ['Frequently', 'Non-Travel', 'Rarely'])\n",
    "\n",
    "    # =====================================\n",
    "    # Calculating HRA, DA, PF and Gross Salary\n",
    "    # =====================================\n",
    "    hra = salary * 0.25  # 25% of Salary\n",
    "    da = salary * 0.2    # 20% of Salary\n",
    "    pf = salary * 0.15   # 15% of Salary\n",
    "    gross_salary = salary + hra + da + pf  # Total Gross Salary\n",
    "    \n",
    "    # Display the calculated values with size and color using markdown\n",
    "    st.markdown(f\"<p style='font-size:20px; color:blue;'>Calculated HRA: <strong>{hra:.2f}</strong></p>\", unsafe_allow_html=True)\n",
    "    st.markdown(f\"<p style='font-size:20px; color:green;'>Calculated DA: <strong>{da:.2f}</strong></p>\", unsafe_allow_html=True)\n",
    "    st.markdown(f\"<p style='font-size:20px; color:orange;'>Calculated PF: <strong>{pf:.2f}</strong></p>\", unsafe_allow_html=True)\n",
    "    st.markdown(f\"<p style='font-size:22px; color:white;'>Calculated Gross Salary: <strong>{gross_salary:.2f}</strong></p>\", unsafe_allow_html=True)\n",
    "        \n",
    "    # Display the calculated values\n",
    "    #st.write(f\"### Calculated HRA: **{hra}**\")\n",
    "    #st.write(f\"### Calculated DA: **{da}**\")\n",
    "    #st.write(f\"### Calculated PF: **{pf}**\")\n",
    "    #st.write(f\"### Calculated Gross Salary: **{gross_salary}**\")\n",
    "\n",
    "    # Create a dictionary of the user input\n",
    "    user_data = {\n",
    "    'Department': department,\n",
    "    'Position': position,\n",
    "    'Education': education,\n",
    "    'Region': region,\n",
    "    'Recruitment_channel': recruitment_channel,\n",
    "    'Years_of_trainings': years_of_trainings,\n",
    "    'Years_of_Experience': years_of_experience,\n",
    "    'Previous_Year_Rating': previous_year_rating,\n",
    "    'KPIs_met >80%': kpis_met,\n",
    "    'Awards_Won': awards_won,\n",
    "    'Avg_Training_Score': avg_training_score,\n",
    "    'Gender': gender,\n",
    "    'Marital_Status': marital_status,\n",
    "    'Dependents': dependents,\n",
    "    'Age': age,\n",
    "    'Salary': salary,\n",
    "    'HRA': hra, # Add calculated HRA\n",
    "    'DA': da, # Add calculated HRA\n",
    "    'PF': pf, # Add calculated HRA\n",
    "    'Gross_Salary': gross_salary, # Add calculated HRA\n",
    "    'Insurance': insurance,\n",
    "    'Over_Time': over_time,\n",
    "    'Business_Travel': business_travel\n",
    "}\n",
    "    \n",
    "    # Convert the user input into a pandas DataFrame\n",
    "    input_df = pd.DataFrame([user_data])\n",
    "    \n",
    "    # Display user input data\n",
    "    st.write(\"### User Input\")\n",
    "    st.write(input_df)\n",
    "    \n",
    "    # Prediction button\n",
    "    if st.button('Predict'):\n",
    "        # Preprocess the input data and predict Attrition\n",
    "        attrition_prob = attrition_model.predict_proba(input_df)[:, 1]  # Probability for class 1 (Attrition = Yes)\n",
    "        promotion_prob = promotion_model.predict_proba(input_df)[:, 1]  # Probability for class 1 (Promotion = Yes)\n",
    "    \n",
    "        # Display results\n",
    "        st.write(f\"The **Attrition** probability is: **{attrition_prob[0] * 100:.2f}%**\")\n",
    "        st.write(f\"The **Promotion** probability is: **{promotion_prob[0] * 100:.2f}%**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c69087-74dc-44a5-ba66-6a4dce794a9f",
   "metadata": {},
   "source": [
    "# <font color='Green'>**_Thank you!_**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4aa28-3cdf-4a06-9602-92d8e8bbfa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb31248-5e8c-47aa-9c04-04cf38a72607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
